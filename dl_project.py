# -*- coding: utf-8 -*-
"""DL-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LbdOG9ElvD0DyfuyG1Y5UPjUjNFWwkqE
"""

#!unzip archivePokemons.zip

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import random
import time

#from google.colab import drive

#drive.mount('/content/drive')

#directory = os.chdir('/content/drive/MyDrive/PokemonsData')

def read_data():
    pokemons_dataset = '/PokemonData'
    labels = os.listdir(pokemons_dataset)
    num_classes = len(labels)
    print("Pokemons in dataset: ",labels)
    print("Num of different pokemons:",num_classes)

""""
#visualize some pokemon
image_path = 'PokemonData/Bulbasaur/00000000.png'
image = mpimg.imread(image_path)
plt.imshow(image)

from tensorflow.keras.preprocessing.image import load_img, img_to_array

def transform_dataset(path_to_data, labels):
    dataset = []
    count = 0
    for label in labels:
        curr_pokemon = os.path.join(path_to_data, label) #navigate tu folder of specific pokemon
        for image in os.listdir(curr_pokemon): #iterate throught images of current pokemon
            try:
                curr_image = load_img(os.path.join(curr_pokemon, image), target_size=(150,150))
                curr_image = img_to_array(curr_image) #convert image to np_array in range (0,255)
                curr_image = curr_image / 255.0 #data normalization to range (0,1)
                dataset.append((curr_image, count)) #pair of image and its label coded to number
            except:
                pass
        print(f'\rCompleted transformations for pokemon: {label}',end='') #processing completed for the pokemon with current label
        count +=1
    random.shuffle(dataset)
    X, y = zip(*dataset)
    return np.array(X), np.array(y)

start_time = time.time()
pokemons_images, pokemons_labels = transform_dataset(pokemons_dataset, labels)
end_time = time.time()
execution_time = end_time - start_time  # compute total time of data transformation
print("Execution time: ", execution_time, "seconds")

pokemons_labels[0]

plt.figure(figsize = (15 , 9)) #visualization first 15 pokemons in transformed dataset
n = 0
for i in range(15):
    n += 1
    plt.subplot(5 , 5, n)
    plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)
    plt.title(f'Label: {labels[pokemons_labels[i]]}')
    plt.imshow(pokemons_images[i])

pokemons_images[0].shape

from sklearn.model_selection import train_test_split
#random_state so we always have the same separation of data for training and test
X_train, X_test, y_train, y_test = train_test_split(pokemons_images, pokemons_labels, test_size=0.33, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization

from tensorflow.keras.applications import DenseNet201 #Dense connected convolutional network

base_model = DenseNet201(include_top = False,
                         weights = 'imagenet',
                         input_shape = (150, 150, 3))

for layer in base_model.layers[:675]:
    layer.trainable = False

for layer in base_model.layers[675:]:
    layer.trainable = True

#CNN model arhitecture
model = Sequential()
model.add(base_model)
model.add(Conv2D(75, kernel_size = (3,3), strides = 1, input_shape=(150, 150, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2), strides=2, padding="same"))
model.add(BatchNormalization())

model.add(Conv2D(50, kernel_size = (3,3), strides = 1, padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2), strides=2, padding="same"))
model.add(BatchNormalization())

model.add(Conv2D(25, kernel_size = (3,3), strides = 1, padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2), strides=2, padding="same"))
model.add(BatchNormalization())

model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units = num_classes, activation='softmax'))

model.summary()

from tensorflow.keras.optimizers import SGD

base_model.trainable = True

history = model.compile(optimizer = SGD(learning_rate = 0.001),
             loss = 'sparse_categorical_crossentropy',
             metrics = ['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)

num_epochs = 50
model.fit(X_train, y_train, epochs = num_epochs, callbacks = [early_stopping], verbose=1, validation_data=(X_test, y_test))

"""
read_data()